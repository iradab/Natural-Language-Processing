{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "type_extraction_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmuyW9YNFxP1"
      },
      "source": [
        "## Extract type facts from a Wikipedia file\n",
        "\n",
        "\n",
        "### === Purpose ===\n",
        "\n",
        "The goal of this lab is to extract the class to which an entity belongs from Wikipedia.\n",
        "For example, given the Wikipedia article about Leicester:\n",
        "\n",
        "    Leicester is a small city in England\n",
        "    \n",
        "the goal is to extract:\n",
        "\n",
        "    Leicester TAB city\n",
        "\n",
        "\n",
        "### === Provided Data ===\n",
        "\n",
        "We provide:\n",
        "\n",
        "1. a preprocessed version of the Simple Wikipedia (`wikipedia-first.txt`), which looks like above\n",
        "2. a template for your code, `extractor.py`\n",
        "3. a gold standard sample (`gold-standard-sample.tsv`).\n",
        "\n",
        "\n",
        "### === Task ===\n",
        "\n",
        "Complete the `extract_type()` function so that it extracts the type of the article entity from the content.\n",
        "For example, for a content of \"Leicester is a beautiful English city in the UK\", it should return \"city\".\n",
        "Exclude terms that are too abstract (\"member of...\", \"way of...\"), and try to extract exactly the noun(s).\n",
        "You can also skip articles (e.g. `return None`) if you are not sure or if the text does not contain any type.\n",
        "In order to ensure a fair evaluation, do not use any non-standard Python libraries except `nltk` (`pip install nltk`).\n",
        "\n",
        "Input:\n",
        "\n",
        "April\n",
        "April is the fourth month of the year with 30 days.\n",
        "\n",
        "Output:\n",
        "April TAB month\n",
        "\n",
        "\n",
        "### === Development and Testing ===\n",
        "\n",
        "We provide a certain number of gold samples for validating your model.\n",
        "Finally, we calculate a F1 score using following equation:\n",
        "\n",
        "`F1 = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)`\n",
        "\n",
        "with `beta = 0.5`, putting more weight on precision in that way.\n",
        "\n",
        "\n",
        "### === Submission ===\n",
        "\n",
        "1. Take your code, any necessary resources to run the code, and the output of your code on the test dataset (no need to put the other datasets!)\n",
        "2. ZIP these files in a file called `firstName_lastName.zip`\n",
        "3. submit it here before the deadline announced during the lab:\n",
        "\n",
        "\n",
        "https://www.dropbox.com/request/n1pxRxyUHuq9w1ewOzWB\n",
        "\n",
        "\n",
        "### === Contact ===\n",
        "\n",
        "If you have any additional questions, you can send an email to: nedeljko.radulovic@telecom-paris.fr\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arrp2SohFxP6"
      },
      "source": [
        "\"\"\"\n",
        "Don't modify this code.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "class Page:\n",
        "    '''\n",
        "    This class is used to store title and content of a wiki page\n",
        "    '''\n",
        "    __author__ = \"Jonathan Lajus\"\n",
        "\n",
        "    def __init__(self, title, content):\n",
        "        self.content = content\n",
        "        self.title = title\n",
        "        if sys.version_info[0] < 3:\n",
        "            self.title = title.decode(\"utf-8\")\n",
        "            self.content = content.decode(\"utf-8\")\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return isinstance(other, self.__class__) and self.title == other.title and self.content == other.content\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not self.__eq__(other)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.title, self.content))\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'Wikipedia page: \"' + (self.title.encode(\"utf-8\") if sys.version_info[0] < 3 else self.title) + '\"'\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "    def _to_tuple(self):\n",
        "        return (self.title, self.content)\n",
        "\n",
        "\n",
        "class Parsy:\n",
        "    '''\n",
        "    Parse a Wikipedia file, return page objects\n",
        "    '''\n",
        "    __author__ = \"Jonathan Lajus\"\n",
        "\n",
        "    def __init__(self, wikipediaFile):\n",
        "        self.file = wikipediaFile\n",
        "\n",
        "    def __iter__(self):\n",
        "        title, content = None,\"\"\n",
        "        with open(self.file, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line and title is not None:\n",
        "                    yield Page(title, content.rstrip())\n",
        "                    title, content = None,\"\"\n",
        "                elif title is None:\n",
        "                    title = line\n",
        "                elif title is not None:\n",
        "                    content += line + \" \"\n",
        "\n",
        "    \n",
        "def eval_f1(gold_file, pred_file):\n",
        "\n",
        "    # Dictionaries\n",
        "    goldstandard = dict()\n",
        "    student = dict()\n",
        "\n",
        "    # Reading first file\n",
        "    with open(gold_file, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            temp = line.split(\"\\t\")\n",
        "            if len(temp) != 2:\n",
        "                print(\"The line:\", line, \"has an incorrect number of tabs\")\n",
        "            else:\n",
        "                if temp[0] in goldstandard:\n",
        "                    print(temp[0], \" has two solutions\")\n",
        "                goldstandard[temp[0]] = str.lower(temp[1])\n",
        "\n",
        "    # Reading second file\n",
        "    with open(pred_file, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            temp = line.split(\"\\t\")\n",
        "            if len(temp) != 2:\n",
        "                if not debug:\n",
        "                    print(\"Comment :=>> The line: '\", line, \"' has an incorrect number of tabs\")\n",
        "                else:\n",
        "                    print(\"The line: '\", line, \"' has an incorrect number of tabs\")\n",
        "            else:\n",
        "                if temp[0] in student:\n",
        "                    if not debug:\n",
        "                        print(\"Comment :=>>\", temp[0], \"has two solutions\")\n",
        "                    else:\n",
        "                        print(temp[0], \" has two solutions\")\n",
        "                student[temp[0]] = str.lower(temp[1])\n",
        "\n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    for key in student:\n",
        "        if key in goldstandard:\n",
        "            if student[key] == goldstandard[key]:\n",
        "                true_pos += 1\n",
        "            else:\n",
        "                false_pos += 1\n",
        "                print(\"You got\", key, \"wrong. Expected output: \", goldstandard[key], \",given:\", student[key])\n",
        "\n",
        "    for key in goldstandard:\n",
        "        if key not in student:\n",
        "            false_neg += 1\n",
        "            print(\"No solution was given for\", key)\n",
        "\n",
        "    if true_pos + false_pos != 0:\n",
        "        precision = float(true_pos) / (true_pos + false_pos) * 100.0\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    if true_pos + false_neg != 0:\n",
        "        recall = float(true_pos) / (true_pos + false_neg + false_pos) * 100.0\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    beta = 0.5\n",
        "\n",
        "    if precision + recall != 0.0:\n",
        "        f05 = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
        "    else:\n",
        "        f05 = 0.0\n",
        "\n",
        "    # grade = 0.75 * precision + 0.25 * recall\n",
        "    grade = f05\n",
        "\n",
        "    print(\"Comment :=>>\", \"Precision:\", precision)\n",
        "    print(\"Comment :=>>\", \"Recall:\", recall)\n",
        "    print(\"Simulated Grade (F0.5) :=>>\", grade)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VET3L2JfFxP_"
      },
      "source": [
        "# a simplified wiki page document\n",
        "wiki_file = '/content/wikipedia-first.txt'\n",
        "# some gold samples for validation\n",
        "gold_file = '/content/gold-standard-sample.tsv'\n",
        "# predicted results generated by your model\n",
        "# you are supposed to submit this file\n",
        "result_file = 'results.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXfC3i_MHuVY",
        "outputId": "67ce6ca9-4568-4dea-da28-ff9f82ddfd2b"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBsy3rYUIAl7"
      },
      "source": [
        "def rule(grammar,tag):\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    result = cp.parse(tag)\n",
        "    docs = []\n",
        "    for subtree in result.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "        docs.append(\" \".join([a for (a,b) in subtree.leaves()]))\n",
        "    if (len(docs) != 0):                \n",
        "        text2 = word_tokenize(docs[0]) \n",
        "        tag = nltk.pos_tag(text2)\n",
        "        if (tag[len(text2)-1][1] == 'NN' or tag[len(text2)-1][1] == 'NNS' or tag[len(text2)-1][1] == 'NNP' or tag[len(text2)-1][1] == 'NNPS'):\n",
        "            return (text2[len(text2)-1])\n",
        "        else:\n",
        "            return 0\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi8HaVG_FxP_"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def extract_type(wiki_page):\n",
        "    '''\n",
        "\n",
        "    :param wiki_page is an object contains a title and the first sentence from its wiki page.\n",
        "    :return:\n",
        "    '''\n",
        "    title = wiki_page.title\n",
        "    content = wiki_page.content\n",
        "\n",
        "    res = []\n",
        "    for i in range(0,len(content)):\n",
        "        if (content[i].isalnum() == True or content[i] == ' ' or content[i] == \"'\" or (i== len(content)-1)):\n",
        "            res.append(content[i])\n",
        "    content = \"\".join(res)\n",
        "\n",
        "    # Code goes here\n",
        "    text = word_tokenize(content)\n",
        "    tag = nltk.pos_tag(text)\n",
        "    \n",
        "    there_is_of = False\n",
        "    for i in range(0,len(text)):\n",
        "        if (tag[i][0] == 'of'):\n",
        "            there_is_of = True\n",
        "            content = content.replace(tag[i][0]+' ','')\n",
        "        if (tag[i][1] == 'RP' or tag[i][1] == 'FW' or tag[i][1] == 'CD'): # \n",
        "            content = content.replace(tag[i][0]+' ','')\n",
        "        if (tag[i][1] == 'POS' or tag[i][0] == \"'s\"):\n",
        "            content = content.replace(tag[i-1][0],'')\n",
        "            content = content.replace(tag[i][0],'')\n",
        "\n",
        "    text = word_tokenize(content)\n",
        "    tag = nltk.pos_tag(text)\n",
        "\n",
        "\n",
        "    grammars = []\n",
        "    grammar1 = \"NP: {<DT>?<NN|NNS|NNP|NNPS>*<MD>?<RB>?<VB|VBD|VBN|VBP|VBZ>+<DT>?<RBS|RBR>?<JJ|JJR|VBG|JJS|PRP$>*<NN|NNS>*}\"\n",
        "    grammar2 = \"NP: {<DT>?<NN|NNS|NNP|NNPS>*<MD>?<RB>?<VB|VBD|VBN|VBP|VBZ>+<TO><RB>?<VB|VBD|VBN|VBP|VBZ>+<DT>?<RBS|RBR>?<JJ|JJR|VBG|JJS|PRP$>*<NN|NNS>*}\" # could, will\n",
        "    grammar3 = \"NP: {<DT>?<NN|NNS|NNP|NNPS>*<RB>?<VB|VBD|VBN|VBP|VBZ>*<DT>?<RBS|RBR>?<JJ|JJR|VBG|JJS|PRP$>*<NN|NNS>*}\"\n",
        "\n",
        "    grammars.append(grammar1)\n",
        "    grammars.append(grammar2)\n",
        "    grammars.append(grammar3)\n",
        "\n",
        "    for i in range(0,len(grammars)):\n",
        "        answer = rule(grammars[i],tag)\n",
        "        if  (answer != 0):\n",
        "            return answer\n",
        "\n",
        "    return None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfIre9Q_FxQA",
        "outputId": "9428a3f7-7baf-4b15-da13-9f1bfe79d9b6"
      },
      "source": [
        "def run():\n",
        "    '''\n",
        "    First, extract types from each sentence in the wiki file\n",
        "    Next, use gold samples to evaluate your model\n",
        "    :return:\n",
        "    '''\n",
        "    with open(result_file, 'w', encoding=\"utf-8\") as output:\n",
        "        for page in Parsy(wiki_file):\n",
        "            typ = extract_type(page)\n",
        "            if typ:\n",
        "                output.write(page.title + \"\\t\" + typ + \"\\n\")\n",
        "\n",
        "    # Evaluate on some gold samples for checking your model\n",
        "    eval_f1(gold_file, result_file)\n",
        "\n",
        "\n",
        "run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You got Army wrong. Expected output:  military\n",
            " ,given: part\n",
            "\n",
            "You got Bath wrong. Expected output:  thing\n",
            " ,given: people\n",
            "\n",
            "You got Hillary Rodham Clinton wrong. Expected output:  senator\n",
            " ,given: junior\n",
            "\n",
            "You got Tropical cyclone wrong. Expected output:  weather\n",
            " ,given: name\n",
            "\n",
            "You got Ritual wrong. Expected output:  actions\n",
            " ,given: people\n",
            "\n",
            "You got Medusa (animal) wrong. Expected output:  animals\n",
            " ,given: forms\n",
            "\n",
            "You got Mud wrong. Expected output:  mixture\n",
            " ,given: dirt\n",
            "\n",
            "You got Artillery wrong. Expected output:  guns\n",
            " ,given: word\n",
            "\n",
            "You got Communication Studies wrong. Expected output:  study\n",
            " ,given: college\n",
            "\n",
            "You got Phoenicia wrong. Expected output:  civilization ,given: civilization\n",
            "\n",
            "Comment :=>> Precision: 88.88888888888889\n",
            "Comment :=>> Recall: 88.88888888888889\n",
            "Simulated Grade (F0.5) :=>> 88.88888888888889\n"
          ]
        }
      ]
    }
  ]
}